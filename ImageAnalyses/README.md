### Code for NARPS data analysis

[![DOI](https://zenodo.org/badge/85984198.svg)](https://zenodo.org/badge/latestdoi/85984198) [![CircleCI](https://circleci.com/gh/poldrack/narps.svg?style=svg)](https://circleci.com/gh/poldrack/narps) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/c35f17b180aa4b1e8cbd33b9b1473c3e)](https://www.codacy.com/app/poldrack/narps?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=poldrack/narps&amp;utm_campaign=Badge_Grade) [![Coverage Status](https://coveralls.io/repos/github/poldrack/narps/badge.svg?branch=master)](https://coveralls.io/github/poldrack/narps?branch=master)

#### Setup

In order to run this code, you must obtain the URL for the original data from Russ Poldrack - these data will be made publicly available upon publication of the paper.

The required data (all contained in the unzipped ```orig``` directory) are:

-   thresholded and unthresholded images for each team/hypothesis (teams excluded from the main analysis are included in the ```rejected``` directory)

-   Metadata files:
    -   ```analysis_pipelines_SW.xlsx``` (information about analysis pipelines)
    -   ```narps_neurovault_images_details.csv``` (information about images)
    -]  ```narps_results.xlsx``` (information about team decisions)

In addition, there is a set of template files (redistributed from the FSL distribution) contained in the ```templates``` directory:
    -   ```templates/MNI152_T1_2mm.nii.gz```
    -   ```templates/MNI152_T1_2mm_brain_mask.nii.gz```

#### Dockerized analysis pipeline

To run the full analysis pipeline using Docker, do the following:

-   Install the [Docker client](https://docs.docker.com/install/)
-   set the following environment variables:
    -   ```NARPS_BASEDIR```: location for the data and results  ## NOT SURE IF DATA SHOULD BE DWNLOADED FIRST
    -   ```DATA_URL```: URL for the data
-   clone the present repository and cd to directory ImageAnalyses
-   use the following command to run the full pipeline: ```make run-all```

This performs the following steps:

-   Preparation of the data for analysis(using narps.py)
-   Preparation of the metadata (using PrepareMetadata.py)
-   Analysis of the maps (using AnalyzeMaps.py)
-   Analysis of the decisions (using AnalyzeDecisions.Rmd)
-   Consensus analysis across teams (using ConsensusAnalysis.py)

The outputs can be found in the subdirectories of NARPS_BASEDIR:
-   ```maps``` - all of the intermediate maps generated by preprocessing
-   ```figures``` - figures generated by the analysis code
-   ```metadata``` - additional metadata files generated by the preparation code
-   ```cached``` - the cached narps structure 

This will use the latest version of the docker image from Dockerhub.  If you wish to build the Docker image locally, you should change the DOCKER_USERNAME variable in the Makefile to your own username, and then run ```make docker-build```.

#### Reproducibility

We have attempted to maximize the reproducibility of the analyses in this project as follows:

-   *Python/UNIX*: All software versions for UNIX and Python packages are pinned in the Dockerfile
-   *R*: R is tricky because it doesn't provide a straightforward way to specify versions of libraries.  We use a package called [checkpoint](https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html), which downloads the versions of all necessary packages as of a specific date (which we have set to 2019-07-16).  The checkpoint packages analyzes R code in the project and downloads any libraries that are loaded by the code; unfortunately it doesn't read Rmd files, so we create a separate file called R_libraries.R that contains the imports needed to run the Rmd file, and which will be detected by checkpoint such that those libraries are loaded automatically.  This is generated automatically when the analysis is run, and can also be generated using ```make get-R-packages```.

### Local execution

Execution via Docker is recommended, but the analysis can also be run locally, using ```make run-all-local``` - this will require that you have all of the various requirements in place, which must be inferred from the Dockerfile.


### Simulation mode

In order to validate the analysis stream, we have included the ability to run the entire analysis stream on simulated data.  This requires that the entire analysis (e.g. run all) has first been run in regular mode, because the simulated data are generated on the basis of the consensus analysis results. The data are generated so that most teams will be correlated, but four teams will be anticorrelated and four teams will be pure noise, and 12 teams have higher variance than the others.  See the code in narps.py for more details.

To run in simulation model, you should use ```python narps.py -s```.  The specified basedir should be the one in which you have already run the full analysis.  A new base directory, with "_simulated" appended to the original directory name, will be generated and the new data will be generated in that directory.  The subsequent analysis methods can then be applied to that new simulated directory structure.  This entire process can be executed using Docker via ```make run-simulated```.